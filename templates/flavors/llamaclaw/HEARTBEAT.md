# HEARTBEAT.md — LlamaClaw

## Local Model Status
- Check if Ollama/llama.cpp is running
- Verify which models are loaded and responsive
- Check GPU/memory usage — alert if VRAM pressure is high

## Quality Check
- If quality logging is enabled, review recent inference quality
- Flag any degradation

## Community Watch
- Weekly check: any notable new Llama fine-tunes or releases on Hugging Face

## Quiet Hours
- Between 23:00–07:00: only alert if the local inference server crashes
